{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Muxev71DTUtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b64559-8ade-4d29-90f8-08347d71005d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Trovato /content/drive/MyDrive/dataset.zip. Inizio estrazione...\n",
            "‚úÖ Estrazione completata!\n"
          ]
        }
      ],
      "source": [
        "########CELLA 1###############\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# 1. Monta Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Configurazione Percorsi\n",
        "# Assumiamo che il file sia nella root del tuo Drive.\n",
        "# Se √® in una sottocartella, modifica in: '/content/drive/MyDrive/NOME_CARTELLA/dataset.zip'\n",
        "zip_path = '/content/drive/MyDrive/dataset.zip'\n",
        "extract_to = '/content/dataset_unzipped'\n",
        "\n",
        "# 3. Estrazione\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"Trovato {zip_path}. Inizio estrazione...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"‚úÖ Estrazione completata!\")\n",
        "else:\n",
        "    print(f\"‚ùå ERRORE: Non trovo il file '{zip_path}'.\")\n",
        "    print(\"Controlla di averlo caricato su Drive e che il nome sia esattamente 'dataset.zip' (tutto minuscolo).\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############CELLA 2#################\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "class BookCoverDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None, class_to_idx=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Percorso al file CSV.\n",
        "            root_dir (string): Directory che contiene le immagini (224x224).\n",
        "            transform (callable, optional): Trasformazioni (Tensor, Normalize).\n",
        "        \"\"\"\n",
        "        # Lettura CSV con i parametri corretti scoperti prima (sep=; encoding=ISO...)\n",
        "        self.df = pd.read_csv(csv_file, sep=';', encoding='ISO-8859-1', header=0, on_bad_lines='warn')\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Ordina le classi per garantire coerenza\n",
        "        self.classes = sorted(self.df['Category'].unique())\n",
        "\n",
        "        # Mappa Stringa -> Intero\n",
        "        if class_to_idx is None:\n",
        "            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "        else:\n",
        "            self.class_to_idx = class_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Recupera nome file e costruisce percorso\n",
        "        img_name = str(self.df.iloc[idx]['Filename'])\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "\n",
        "        # Caricamento Immagine\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except (OSError, FileNotFoundError):\n",
        "            # Gestione immagine mancante (crea immagine nera)\n",
        "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "\n",
        "        # Label\n",
        "        label_str = self.df.iloc[idx]['Category']\n",
        "        label = self.class_to_idx[label_str]\n",
        "\n",
        "        # Trasformazioni\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# --- CONFIGURAZIONE TRASFORMAZIONI ---\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        # 1. RandomResizedCrop: Taglia pezzi casuali e li riporta a 224x224.\n",
        "        # Costringe la rete a guardare i dettagli. scale=(0.8, 1.0) significa che prende almeno l'80% dell'immagine.\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "\n",
        "        # 2. ColorJitter: Variazione di luminosit√† e contrasto (fondamentale per scansioni diverse)\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "\n",
        "        transforms.RandomRotation(15),\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)), # Nel validation NON facciamo augmentation\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "YFPreKw1Tb8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################CELLA 3####################\n",
        "# Cerchiamo i percorsi corretti navigando tra le cartelle estratte\n",
        "base_search_path = '/content/dataset_unzipped'\n",
        "csv_path = None\n",
        "img_dir = None\n",
        "\n",
        "print(\"üîç Scansione cartelle in corso...\")\n",
        "\n",
        "for root, dirs, files in os.walk(base_search_path):\n",
        "    # Cerchiamo il CSV di training\n",
        "    if \"book30-listing-train.csv\" in files:\n",
        "        csv_path = os.path.join(root, \"book30-listing-train.csv\")\n",
        "        print(f\"   -> CSV Trovato: {csv_path}\")\n",
        "\n",
        "    # Cerchiamo la cartella specifica delle immagini\n",
        "    if \"224x224\" in dirs:\n",
        "        img_dir = os.path.join(root, \"224x224\")\n",
        "        print(f\"   -> Cartella Immagini Trovata: {img_dir}\")\n",
        "\n",
        "# --- VERIFICA E CREAZIONE DATASET ---\n",
        "if csv_path and img_dir:\n",
        "    print(\"\\n‚úÖ File trovati! Creazione Dataset in corso...\")\n",
        "\n",
        "    # Creiamo il dataset\n",
        "    train_dataset = BookCoverDataset(\n",
        "        csv_file=csv_path,\n",
        "        root_dir=img_dir,\n",
        "        transform=data_transforms['train']\n",
        "    )\n",
        "\n",
        "    # TEST RAPIDO\n",
        "    print(f\"Dataset caricato correttamente con {len(train_dataset)} libri.\")\n",
        "    print(f\"Numero di classi (Generi): {len(train_dataset.classes)}\")\n",
        "\n",
        "    # Verifica GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"\\nüñ•Ô∏è Device attivo: {device}\")\n",
        "    if device.type == 'cuda':\n",
        "        print(\"üöÄ Perfetto! La GPU NVIDIA T4 √® pronta a spingere.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è ATTENZIONE: Stai usando la CPU. Vai su Modifica -> Impostazioni Notebook -> Hardware Accelerator -> T4 GPU\")\n",
        "\n",
        "    # Test estrazione di un elemento\n",
        "    img, label = train_dataset[0]\n",
        "    print(f\"\\nTest Shape Tensore: {img.shape} (Deve essere 3, 224, 224)\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ùå ERRORE CRITICO: Non ho trovato i file necessari.\")\n",
        "    print(\"Controlla il contenuto dello zip. Cerco 'book30-listing-train.csv' e una cartella '224x224'.\")"
      ],
      "metadata": {
        "id": "dKMs6qpuTiSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "821be9ae-84a0-4298-bdbe-cadf82476adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Scansione cartelle in corso...\n",
            "   -> CSV Trovato: /content/dataset_unzipped/dataset/book30-listing-train.csv\n",
            "   -> Cartella Immagini Trovata: /content/dataset_unzipped/dataset/title30cat/224x224\n",
            "\n",
            "‚úÖ File trovati! Creazione Dataset in corso...\n",
            "Dataset caricato correttamente con 51300 libri.\n",
            "Numero di classi (Generi): 30\n",
            "\n",
            "üñ•Ô∏è Device attivo: cuda\n",
            "üöÄ Perfetto! La GPU NVIDIA T4 √® pronta a spingere.\n",
            "\n",
            "Test Shape Tensore: torch.Size([3, 224, 224]) (Deve essere 3, 224, 224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############CELLA 4##############\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# 1. Divisione Train / Validation (80% / 20%)\n",
        "total_size = len(train_dataset)\n",
        "train_len = int(0.8 * total_size)\n",
        "val_len = total_size - train_len\n",
        "\n",
        "# random_split mischia gli indici e crea due sotto-dataset\n",
        "train_subset, val_subset = random_split(train_dataset, [train_len, val_len])\n",
        "\n",
        "print(f\"üìä Split completato:\")\n",
        "print(f\"   -> Training Set: {len(train_subset)} immagini\")\n",
        "print(f\"   -> Validation Set: {len(val_subset)} immagini\")\n",
        "\n",
        "# 2. Creazione dei DataLoader (I 'nastri trasportatori' per la GPU)\n",
        "# Batch Size 64 √® ottimale per la GPU T4 di Colab (usa bene la memoria senza esaurirla)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"‚úÖ Dataloaders pronti (Batch size: {BATCH_SIZE})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmRJEa8Zb3sI",
        "outputId": "2a9b89d1-ae40-468f-8d86-0dcad4ba5ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Split completato:\n",
            "   -> Training Set: 41040 immagini\n",
            "   -> Validation Set: 10260 immagini\n",
            "‚úÖ Dataloaders pronti (Batch size: 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############CELLA 5###################\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "\n",
        "def get_model(num_classes=30):\n",
        "    print(\"Scaricamento pesi ResNet50 (ImageNet)...\")\n",
        "    # Scarichiamo la versione pi√π aggiornata dei pesi\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "\n",
        "    # 1. FREEZING: Congeliamo tutti i parametri\n",
        "    # Questo impedisce che durante il training modifichiamo i filtri che sanno gi√† \"vedere\"\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 2. Sostituzione dell'ultimo layer (Fully Connected)\n",
        "    # ResNet50 ha 2048 feature in ingresso all'ultimo layer\n",
        "    num_ftrs = model.fc.in_features\n",
        "\n",
        "    # Creiamo il nuovo layer classificatore.\n",
        "    # Nota: Di default, i nuovi layer hanno requires_grad=True, quindi QUESTI verranno addestrati.\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(num_ftrs, 512), # Strato intermedio per imparare combinazioni complesse\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),          # Dropout per evitare overfitting (tecnica standard)\n",
        "        nn.Linear(512, num_classes) # Output finale: 30 probabilit√†\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Istanziamo il modello e lo spostiamo sulla GPU\n",
        "model = get_model(num_classes=len(train_dataset.classes))\n",
        "model = model.to(device) # Sposta tutto sulla GPU T4\n",
        "\n",
        "print(\"\\nü§ñ Modello ResNet50 caricato e modificato per 30 classi.\")\n",
        "print(\"   -> Backbone (corpo): Congelato ‚ùÑÔ∏è\")\n",
        "print(\"   -> Head (testa): Pronta per l'addestramento üî•\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KhCNduscRN-",
        "outputId": "5e766cf3-48d7-4b5d-e26a-e1eb50e3a8dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaricamento pesi ResNet50 (ImageNet)...\n",
            "\n",
            "ü§ñ Modello ResNet50 caricato e modificato per 30 classi.\n",
            "   -> Backbone (corpo): Congelato ‚ùÑÔ∏è\n",
            "   -> Head (testa): Pronta per l'addestramento üî•\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################CELLA 6#################\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# --- CONFIGURAZIONE TRAINING ---\n",
        "# 1. Loss Function: CrossEntropy √® standard per classificazione multi-classe\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 2. Optimizer: Adam √® solitamente pi√π veloce a convergere rispetto a SGD\n",
        "# Passiamo solo model.fc.parameters() perch√© il resto della rete √® congelato!\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# 3. Numero di Epoche (Quante volte la rete vede tutto il dataset)\n",
        "# Inizia con 5 epoche per vedere se funziona, poi puoi aumentare a 10 o 20\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# --- FUNZIONE DI TRAINING ---\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Ogni epoca ha una fase di training e una di validation\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Mette il modello in modalit√† addestramento\n",
        "                dataloader = train_loader\n",
        "            else:\n",
        "                model.eval()   # Mette il modello in modalit√† valutazione (congela dropout, ecc.)\n",
        "                dataloader = val_loader\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            total_samples = 0\n",
        "\n",
        "            # Iterazione sui batch\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Azzera i gradienti\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward (calcolo predizioni)\n",
        "                # track_grad solo se siamo in training\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + Optimize solo in training\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistiche\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                total_samples += inputs.size(0)\n",
        "\n",
        "            epoch_loss = running_loss / total_samples\n",
        "            epoch_acc = running_corrects.double() / total_samples\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "    time_elapsed = time.time() - start_time\n",
        "    print(f'\\nAddestramento completato in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    return model\n",
        "\n",
        "# --- AVVIO TRAINING ---\n",
        "# Salviamo il modello addestrato nella variabile 'trained_model'\n",
        "print(\"üöÄ Inizio Addestramento...\")\n",
        "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8uWW8uucqHj",
        "outputId": "bdd2ac85-3dc5-4f09-9484-4112e7013b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Inizio Addestramento...\n",
            "\n",
            "Epoch 1/5\n",
            "----------\n",
            "train Loss: 2.9255 Acc: 0.1937\n",
            "val Loss: 2.7283 Acc: 0.2459\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "train Loss: 2.7430 Acc: 0.2343\n",
            "val Loss: 2.6839 Acc: 0.2508\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "train Loss: 2.6790 Acc: 0.2511\n",
            "val Loss: 2.6532 Acc: 0.2644\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "train Loss: 2.6259 Acc: 0.2630\n",
            "val Loss: 2.6454 Acc: 0.2668\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "train Loss: 2.5847 Acc: 0.2738\n",
            "val Loss: 2.6258 Acc: 0.2656\n",
            "\n",
            "Addestramento completato in 23m 39s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############CELLA 7###############\n",
        "# --- FINE TUNING ---\n",
        "\n",
        "print(\"‚ùÑÔ∏è Scongelamento dei pesi del Backbone...\")\n",
        "# 1. Sblocchiamo TUTTI i parametri della rete\n",
        "for param in trained_model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 2. Nuovo Optimizer con Learning Rate MOLTO pi√π basso\n",
        "# Usiamo 1e-4 (0.0001) o 1e-5. Se √® troppo alto, distruggiamo le conoscenze pregresse.\n",
        "# Aggiungiamo weight_decay=1e-4 (o 1e-5)\n",
        "optimizer_ft = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "# 3. Addestriamo per altre epoche (Fine-Tuning)\n",
        "print(\"üöÄ Inizio Fine-Tuning (tutta la rete)...\")\n",
        "finetuned_model = train_model(\n",
        "    trained_model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer_ft,\n",
        "    num_epochs=10 # Proviamo 10 epoche, ci metter√† un po' di pi√π\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "k3Mou0UYgdKN",
        "outputId": "deb319d9-de50-49d4-a7a5-4843f29f89da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùÑÔ∏è Scongelamento dei pesi del Backbone...\n",
            "üöÄ Inizio Fine-Tuning (tutta la rete)...\n",
            "\n",
            "Epoch 1/10\n",
            "----------\n",
            "train Loss: 2.4366 Acc: 0.3097\n",
            "val Loss: 2.4745 Acc: 0.3048\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "train Loss: 2.2381 Acc: 0.3582\n",
            "val Loss: 2.4444 Acc: 0.3186\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "train Loss: 2.0616 Acc: 0.4035\n",
            "val Loss: 2.4568 Acc: 0.3175\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "train Loss: 1.8960 Acc: 0.4415\n",
            "val Loss: 2.4640 Acc: 0.3139\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3077941597.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 3. Addestriamo per altre epoche (Fine-Tuning)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üöÄ Inizio Fine-Tuning (tutta la rete)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m finetuned_model = train_model(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2683725906.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;31m# Statistiche\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mtotal_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}