{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270afca",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-16T22:23:57.875376Z",
     "iopub.status.busy": "2025-12-16T22:23:57.875056Z",
     "iopub.status.idle": "2025-12-16T22:24:11.366563Z",
     "shell.execute_reply": "2025-12-16T22:24:11.365621Z"
    },
    "papermill": {
     "duration": 13.495986,
     "end_time": "2025-12-16T22:24:11.367906",
     "exception": false,
     "start_time": "2025-12-16T22:23:57.871920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#IMPORTS & LOGS\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, Sampler\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "\n",
    "#Settings\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'img_size': 224,\n",
    "    'num_workers': 2,\n",
    "    'batch_size': 720,     \n",
    "    'n_trials': 20,        \n",
    "    'epochs_per_trial': 6, \n",
    "    'log_file': 'training_log.txt'\n",
    "}\n",
    "\n",
    "#Logs\n",
    "def log(message):\n",
    "    print(message)\n",
    "    with open(CONFIG['log_file'], \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "\n",
    "#Clears old existing log\n",
    "if os.path.exists(CONFIG['log_file']):\n",
    "    os.remove(CONFIG['log_file'])\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    log(f\" Seed set at {seed}\")\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log(f\" Device: {device} | GPU Count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    log(f\"Using Multi-GPU: Effective Batch for GPU ~= {CONFIG['batch_size'] // torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a3200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T22:24:11.373044Z",
     "iopub.status.busy": "2025-12-16T22:24:11.372479Z",
     "iopub.status.idle": "2025-12-16T22:24:47.690169Z",
     "shell.execute_reply": "2025-12-16T22:24:47.689286Z"
    },
    "papermill": {
     "duration": 36.322748,
     "end_time": "2025-12-16T22:24:47.692751",
     "exception": false,
     "start_time": "2025-12-16T22:24:11.370003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DATASET & SAMPLER\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(CONFIG['img_size'], scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "class BookCoverDataset(Dataset):\n",
    "    def __init__(self, df, root_dir, transform=None, class_to_idx=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(self.df['Category'].unique())\n",
    "        self.class_to_idx = class_to_idx if class_to_idx else {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, str(row['Filename']))\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (CONFIG['img_size'], CONFIG['img_size']))\n",
    "        \n",
    "        label = self.class_to_idx[row['Category']]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, dataset_df, batch_size):\n",
    "        self.df = dataset_df\n",
    "        self.batch_size = batch_size\n",
    "        self.classes = sorted(self.df['Category'].unique())\n",
    "        self.n_classes = len(self.classes)\n",
    "        \n",
    "        # Check batch division\n",
    "        if self.batch_size % self.n_classes != 0:\n",
    "             raise ValueError(f\"Batch size {self.batch_size} non divisible by {self.n_classes} classes.\")\n",
    "\n",
    "        self.samples_per_class = self.batch_size // self.n_classes\n",
    "        \n",
    "        self.class_indices = {c: [] for c in self.classes}\n",
    "        for idx, row in self.df.iterrows():\n",
    "            self.class_indices[row['Category']].append(idx)\n",
    "            \n",
    "        self.min_samples = min(len(v) for v in self.class_indices.values())\n",
    "        self.n_batches = self.min_samples // self.samples_per_class\n",
    "        \n",
    "        log(f\"Sampler: {self.samples_per_class} img/class x {self.n_classes} classes = {self.batch_size} BS.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for c in self.classes:\n",
    "            np.random.shuffle(self.class_indices[c])\n",
    "            \n",
    "        for i in range(self.n_batches):\n",
    "            batch = []\n",
    "            for c in self.classes:\n",
    "                batch.extend(self.class_indices[c][i*self.samples_per_class : (i+1)*self.samples_per_class])\n",
    "            np.random.shuffle(batch)\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "#Preparing data\n",
    "# File Search\n",
    "INPUT_DIR = '/kaggle/input'\n",
    "csv_train_path = None\n",
    "img_dir = None\n",
    "\n",
    "for root, dirs, files in os.walk(INPUT_DIR):\n",
    "    if \"book30-listing-train.csv\" in files:\n",
    "        csv_train_path = os.path.join(root, \"book30-listing-train.csv\")\n",
    "    if \"224x224\" in dirs:\n",
    "        img_dir = os.path.join(root, \"224x224\")\n",
    "\n",
    "if not csv_train_path: raise FileNotFoundError(\"CSV Train not found!\")\n",
    "\n",
    "#Load data and split\n",
    "full_df = pd.read_csv(csv_train_path, sep=';', encoding='ISO-8859-1', on_bad_lines='warn')\n",
    "train_df, val_df = train_test_split(full_df, test_size=0.2, stratify=full_df['Category'], random_state=CONFIG['seed'])\n",
    "\n",
    "#Mandatory index reset for the sampler\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "train_ds = BookCoverDataset(train_df, img_dir, transform=data_transforms['train'])\n",
    "val_ds = BookCoverDataset(val_df, img_dir, transform=data_transforms['val'], class_to_idx=train_ds.class_to_idx)\n",
    "\n",
    "log(\"Data loaded and ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61963b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T22:24:47.697343Z",
     "iopub.status.busy": "2025-12-16T22:24:47.697047Z",
     "iopub.status.idle": "2025-12-17T01:13:32.321937Z",
     "shell.execute_reply": "2025-12-17T01:13:32.320762Z"
    },
    "papermill": {
     "duration": 10124.629829,
     "end_time": "2025-12-17T01:13:32.324248",
     "exception": false,
     "start_time": "2025-12-16T22:24:47.694419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#OPTUNA SEARCH\n",
    "\n",
    "def objective(trial):\n",
    "    #Suggested hyperparameters\n",
    "    lr_backbone = trial.suggest_float(\"lr_backbone\", 1e-6, 1e-4, log=True)\n",
    "    lr_head = trial.suggest_float(\"lr_head\", 1e-4, 1e-2, log=True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.2, 0.7, step=0.1)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "    \n",
    "    #Model setup\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    for param in model.parameters(): param.requires_grad = False # Freeze\n",
    "    for param in model.layer4.parameters(): param.requires_grad = True # Partial unfreeze\n",
    "    \n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(512, 30)\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.module.layer4.parameters() if isinstance(model, nn.DataParallel) else model.layer4.parameters(), 'lr': lr_backbone},\n",
    "        {'params': model.module.fc.parameters() if isinstance(model, nn.DataParallel) else model.fc.parameters(), 'lr': lr_head}\n",
    "    ], weight_decay=weight_decay)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # DataLoader, creates everytime the sampler\n",
    "    train_sampler = BalancedBatchSampler(train_df, batch_size=CONFIG['batch_size'])\n",
    "    train_loader = DataLoader(train_ds, batch_sampler=train_sampler, num_workers=CONFIG['num_workers'])\n",
    "    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'])\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    #Brief training loop\n",
    "    for epoch in range(CONFIG['epochs_per_trial']):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        \n",
    "        #Middle Report for pruning\n",
    "        trial.report(accuracy, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "        if accuracy > best_val_acc:\n",
    "            best_val_acc = accuracy\n",
    "\n",
    "    return best_val_acc\n",
    "\n",
    "#Studio Start\n",
    "log(\"\\n Starting Optuna Optimization...\")\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "try:\n",
    "    study.optimize(objective, n_trials=CONFIG['n_trials'])\n",
    "    \n",
    "    log(\"\\n Optuna Finished\")\n",
    "    log(f\"üèÜ Best Value (Acc): {study.best_trial.value:.4f}\")\n",
    "    log(\"üèÜ Best Params:\")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        log(f\"    {key}: {value}\")\n",
    "\n",
    "    # Saving parameters on JSON\n",
    "    import json\n",
    "    with open(\"best_hyperparameters.json\", \"w\") as f:\n",
    "        json.dump(study.best_trial.params, f)\n",
    "    log(\"Parameters saved in 'best_hyperparameters.json'\")\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\"Critical Error during Optuna: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4800241,
     "sourceId": 8124024,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10181.813613,
   "end_time": "2025-12-17T01:13:35.452514",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-16T22:23:53.638901",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
